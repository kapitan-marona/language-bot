from __future__ import annotations
import os
import time
import random
import re
import tempfile
import logging
from telegram import Update
from telegram.ext import ContextTypes

from openai import OpenAI
from config.config import OPENAI_API_KEY

from components.gpt_client import ask_gpt
from components.voice import synthesize_voice
from components.mode import MODE_SWITCH_MESSAGES, get_mode_keyboard
from state.session import user_sessions
from handlers.chat.prompt_templates import get_system_prompt
from components.triggers import CREATOR_TRIGGERS, MODE_TRIGGERS
from components.triggers import is_strict_mode_trigger, is_strict_say_once_trigger

logger = logging.getLogger(__name__)

client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

MAX_HISTORY_LENGTH = 40
RATE_LIMIT_SECONDS = 1.5

LANGUAGE_CODES = {
    "en": "en-US",
    "fr": "fr-FR",
    "de": "de-DE",
    "es": "es-ES",
    "ru": "ru-RU",
    "sv": "sv-SE",
    "fi": "fi-FI",
}

def _sanitize_user_text(text: str, max_len: int = 2000) -> str:
    t = (text or "").strip()
    if len(t) > max_len:
        t = t[:max_len]
    return t

async def _send_voice_or_audio(context: ContextTypes.DEFAULT_TYPE, chat_id: int, file_path: str):
    """Ğ•ÑĞ»Ğ¸ .ogg â€” ÑˆĞ»Ñ‘Ğ¼ ĞºĞ°Ğº voice, Ğ¸Ğ½Ğ°Ñ‡Ğµ (.mp3) â€” ĞºĞ°Ğº audio."""
    if not file_path:
        return
    if file_path.lower().endswith(".ogg"):
        with open(file_path, "rb") as vf:
            await context.bot.send_voice(chat_id=chat_id, voice=vf)
    else:
        with open(file_path, "rb") as af:
            await context.bot.send_audio(chat_id=chat_id, audio=af)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ“Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ message handler
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    chat_id = update.effective_chat.id
    session = user_sessions.setdefault(chat_id, {})

    # Ğ•ÑĞ»Ğ¸ Ğ¶Ğ´Ñ‘Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¾ĞºĞ¾Ğ´ â€” Ğ¾Ñ‚Ğ´Ğ°Ñ‘Ğ¼ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº Ğ¾Ğ½Ğ±Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ³Ğ°
    try:
        stage = session.get("onboarding_stage")
    except Exception:
        stage = None
    if stage == "awaiting_promo":
        from components.onboarding import promo_code_message
        return await promo_code_message(update, context)

    # Rate limiting
    now = time.time()
    last_time = session.get("last_message_time", 0.0)
    if now - last_time < RATE_LIMIT_SECONDS:
        await context.bot.send_message(chat_id=chat_id, text="â³ ĞŸĞ¾Ğ³Ğ¾Ğ´Ğ¸, Ğ´ÑƒĞ¼Ğ°Ñ ğŸ™‚")
        return
    session["last_message_time"] = now

    try:
        # defaults
        session.setdefault("interface_lang", "en")
        session.setdefault("target_lang", "en")
        session.setdefault("level", "A2")
        session.setdefault("mode", "text")
        session.setdefault("style", "casual")

        interface_lang = session["interface_lang"]
        target_lang = session["target_lang"]
        level = session["level"]
        mode = session["mode"]
        style = session["style"]

        # â”€â”€ Ğ²Ñ…Ğ¾Ğ´ÑÑ‰ĞµĞµ: Ğ³Ğ¾Ğ»Ğ¾Ñ Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚
        if update.message and update.message.voice:
            # Whisper
            if not client:
                await context.bot.send_message(chat_id=chat_id, text="âš ï¸ Ğ“Ğ¾Ğ»Ğ¾Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ (Ğ½ĞµÑ‚ API-ĞºĞ»ÑÑ‡Ğ°). ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼.")
                return

            voice_file = await context.bot.get_file(update.message.voice.file_id)
            with tempfile.NamedTemporaryFile(suffix=".ogg", delete=False) as tf:
                await voice_file.download_to_drive(tf.name)
                audio_path = tf.name

            try:
                with open(audio_path, "rb") as f:
                    tr = client.audio.transcriptions.create(
                        model="whisper-1",
                        file=f,
                        response_format="text",
                    )
                user_input = (tr or "").strip()
                logger.info("Whisper recognized text: %r", user_input)
            except Exception:
                logger.exception("[Whisper Error]")
                await context.bot.send_message(chat_id=chat_id, text="â—ï¸ĞÑˆĞ¸Ğ±ĞºĞ° Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ¾Ğ»Ğ¾ÑĞ°. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹ ĞµÑ‰Ñ‘ Ñ€Ğ°Ğ·.")
                user_input = ""
            finally:
                try:
                    os.remove(audio_path)
                except Exception:
                    pass
        else:
            user_input = (update.message.text if update.message else "") or ""

        # Ğ¡Ğ°Ğ½Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
        user_input = _sanitize_user_text(user_input, max_len=2000)
        if not user_input:
            await context.bot.send_message(chat_id=chat_id, text="â—ï¸ĞŸĞ¾Ñ…Ğ¾Ğ¶Ğµ, ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¾. Ğ¡ĞºĞ°Ğ¶Ğ¸ Ñ‡Ñ‚Ğ¾-Ğ½Ğ¸Ğ±ÑƒĞ´ÑŒ ĞµÑ‰Ñ‘ ğŸ™‚")
            return

        # â”€â”€ Ğ Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¾Ğ·Ğ²ÑƒÑ‡ĞºĞ°
        if is_strict_say_once_trigger(user_input, interface_lang):
            last_text = session.get("last_assistant_text")
            if not last_text:
                if interface_lang == "ru":
                    await update.message.reply_text("ĞŸĞ¾ĞºĞ° Ğ¼Ğ½Ğµ Ğ½ĞµÑ‡ĞµĞ³Ğ¾ Ğ¾Ğ·Ğ²ÑƒÑ‡Ğ¸Ñ‚ÑŒ. Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ´Ğ¾Ğ¶Ğ´Ğ¸ÑÑŒ Ğ¼Ğ¾ĞµĞ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, Ğ° Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ¿Ğ¸ÑˆĞ¸ Â«Ğ¾Ğ·Ğ²ÑƒÑ‡ÑŒÂ».")
                else:
                    await update.message.reply_text("I have nothing to voice yet. First wait for my reply, then say â€œvoice itâ€.")
                return

            try:
                voice_path = synthesize_voice(
                    last_text,
                    LANGUAGE_CODES.get(target_lang, "en-US"),
                    level  # ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ² voice.py
                )
                if voice_path:
                    await _send_voice_or_audio(context, chat_id, voice_path)
                    if level in ["A0", "A1", "A2"]:
                        await context.bot.send_message(chat_id=chat_id, text=last_text)
                else:
                    if interface_lang == "ru":
                        await update.message.reply_text("ĞĞµ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ»Ğ¾ÑÑŒ Ğ¾Ğ·Ğ²ÑƒÑ‡Ğ¸Ñ‚ÑŒ. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹ ĞµÑ‰Ñ‘ Ñ€Ğ°Ğ· Ñ‡ÑƒÑ‚ÑŒ Ğ¿Ğ¾Ğ·Ğ¶Ğµ.")
                    else:
                        await update.message.reply_text("Couldnâ€™t generate audio. Please try again later.")
            except Exception:
                logger.exception("[One-shot TTS error]")
                if interface_lang == "ru":
                    await update.message.reply_text("ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ·Ğ²ÑƒÑ‡ĞºĞµ. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ¿Ğ¾Ğ·Ğ¶Ğµ.")
                else:
                    await update.message.reply_text("An error occurred while generating audio. Letâ€™s try later.")
            finally:
                return

        # â”€â”€ ĞŸĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ²
        if is_strict_mode_trigger(user_input, "voice"):
            session["mode"] = "voice"
            msg = MODE_SWITCH_MESSAGES["voice"].get(interface_lang, MODE_SWITCH_MESSAGES["voice"]["en"])
            await update.message.reply_text(msg, reply_markup=get_mode_keyboard("voice", interface_lang))
            return

        if is_strict_mode_trigger(user_input, "text"):
            session["mode"] = "text"
            msg = MODE_SWITCH_MESSAGES["text"].get(interface_lang, MODE_SWITCH_MESSAGES["text"]["en"])
            await update.message.reply_text(msg, reply_markup=get_mode_keyboard("text", interface_lang))
            return

        # ĞœÑĞ³ĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸
        user_text_norm = user_input.lower()
        if any(phrase in user_text_norm for phrase in MODE_TRIGGERS["voice"]):
            if interface_lang == "ru":
                await update.message.reply_text("Ğ•ÑĞ»Ğ¸ Ñ…Ğ¾Ñ‡ĞµÑˆÑŒ Ğ¿ĞµÑ€ĞµĞ¹Ñ‚Ğ¸ Ğ² Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼ â€” Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ½Ğ°Ğ¿Ğ¸ÑˆĞ¸ **Ğ³Ğ¾Ğ»Ğ¾Ñ** ğŸ˜‰", parse_mode="Markdown")
            else:
                await update.message.reply_text("To switch to voice mode, just type **voice** ğŸ˜‰", parse_mode="Markdown")
            return

        if any(phrase in user_text_norm for phrase in MODE_TRIGGERS["text"]):
            if interface_lang == "ru":
                await update.message.reply_text("Ğ§Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿ĞµÑ€ĞµĞ¹Ñ‚Ğ¸ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼, Ğ½Ğ°Ğ¿Ğ¸ÑˆĞ¸ **Ñ‚ĞµĞºÑÑ‚** ğŸ™‚", parse_mode="Markdown")
            else:
                await update.message.reply_text("To switch to text mode, type **text** ğŸ™‚", parse_mode="Markdown")
            return

        # Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¿Ñ€Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ĞµĞ»Ñ
        found_trigger = False
        norm_for_creator = re.sub(r"[^\w\s]", "", user_input.lower())
        for trig in CREATOR_TRIGGERS.get(interface_lang, CREATOR_TRIGGERS["en"]):
            if trig in norm_for_creator:
                found_trigger = True
                break
        if found_trigger:
            if interface_lang == "ru":
                reply_text = "ğŸ¾ ĞœĞ¾Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ĞµĞ»ÑŒ â€” @marrona! Ğ”Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğº ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°Ğ¹ÑÑ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğº Ğ½ĞµĞ¹. ğŸŒ·"
            else:
                reply_text = "ğŸ¾ My creator is @marrona! For feedback or collaboration offers, feel free to contact her directly. ğŸŒ·"
            await update.message.reply_text(reply_text)
            return

        # â”€â”€ Ğ¡Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ´Ğ»Ñ GPT
        history = session.setdefault("history", [])
        system_prompt = get_system_prompt(style, level, interface_lang, target_lang, mode)

        prompt = [{"role": "system", "content": system_prompt}]
        for msg in history:
            prompt.append(msg)
        prompt.append({"role": "user", "content": user_input})

        assistant_reply = await ask_gpt(prompt, "gpt-4o")

        # Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°
        history.append({"role": "user", "content": user_input})
        history.append({"role": "assistant", "content": assistant_reply})
        while len(history) > MAX_HISTORY_LENGTH:
            history.pop(0)

        # â”€â”€ ĞÑ‚Ğ²ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
        if mode == "voice":
            # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ³Ğ¾Ğ»Ğ¾Ñ; Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞµ â€” Ñ„Ğ¾Ğ»Ğ±ÑĞº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼
            try:
                voice_path = synthesize_voice(assistant_reply, LANGUAGE_CODES.get(target_lang, "en-US"), level)
                if voice_path:
                    await _send_voice_or_audio(context, chat_id, voice_path)
                    if level in ["A0", "A1", "A2"]:
                        await context.bot.send_message(chat_id=chat_id, text=assistant_reply)
                else:
                    raise RuntimeError("No TTS data")
            except Exception:
                logger.exception("[Voice reply error]")
                await context.bot.send_message(chat_id=chat_id, text="âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ. Ğ’Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚:\n" + assistant_reply)
            finally:
                session["last_assistant_text"] = assistant_reply
        else:
            await update.message.reply_text(assistant_reply)
            session["last_assistant_text"] = assistant_reply

    except Exception:
        logger.exception("[ĞĞ¨Ğ˜Ğ‘ĞšĞ Ğ² handle_message]")
        await context.bot.send_message(chat_id=chat_id, text="âš ï¸ Ğ§Ñ‚Ğ¾-Ñ‚Ğ¾ Ğ¿Ğ¾ÑˆĞ»Ğ¾ Ğ½Ğµ Ñ‚Ğ°Ğº! ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹ ĞµÑ‰Ñ‘ Ñ€Ğ°Ğ· Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸ Ğ±Ğ¾Ñ‚Ğ° ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ¾Ğ¹ /start.")
